{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30d3e70e-b4ba-429d-a7df-9359d3ad82b8",
   "metadata": {},
   "source": [
    "# Imports segments\n",
    "import module torch, numpy, ....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9a4faba-65bd-4509-a046-03afa49e84ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771c5e2d-700e-42a8-bcad-b859b3d30e0b",
   "metadata": {},
   "source": [
    "# Design of Neural network etc\n",
    "\n",
    "## Exiting Neural Network\n",
    "\n",
    "### Which part of the Neural Network will be freezed\n",
    "### Design of neural network for the specific purpose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "982666a4-1795-4916-8972-ca52601b5171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(net, init_type='kaiming', gain=0.02):\n",
    "    def init_func(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "            if init_type == 'normal':\n",
    "                init.normal_(m.weight.data, 0.0, gain)\n",
    "            elif init_type == 'xavier':\n",
    "                init.xavier_normal_(m.weight.data, gain=gain)\n",
    "            elif init_type == 'kaiming':\n",
    "                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "            elif init_type == 'orthogonal':\n",
    "                init.orthogonal_(m.weight.data, gain=gain)\n",
    "            else:\n",
    "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                init.constant_(m.bias.data, 0.0)\n",
    "        elif classname.find('BatchNorm2d') != -1:\n",
    "            init.normal_(m.weight.data, 1.0, gain)\n",
    "            init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    print('initialize network with %s' % init_type)\n",
    "    net.apply(init_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b0ea56-a7d2-4d34-9f50-d9ca821e0d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block_1(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(conv_block_1, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch_out, ch_out, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22b38216-10be-46ae-ac68-cfea7886fe72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class conv_block_2(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(conv_block_2, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=2, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch_out, ch_out, kernel_size=2, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch_out, ch_out, kernel_size=2, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch_out, ch_out, kernel_size=2, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb55e55e-3810-4f6a-ad57-2288dac9a8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block_3(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(conv_block_3, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe75e852-4444-478d-b4f6-78ee976fa538",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block_5(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(conv_block_5, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=5, stride=1, padding=2, bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch_out, ch_out, kernel_size=5, stride=1, padding=2, bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4db52584-057d-413b-bd58-ce0b4f97ede8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0204, -1.8225, -1.1713],\n",
       "        [-0.1830, -1.2356, -0.5125],\n",
       "        [ 0.2125, -0.3128, -0.3966],\n",
       "        [-0.1998,  1.1097,  1.9656],\n",
       "        [ 1.0736, -0.0634,  0.6217]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class conv_block_7(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(conv_block_7, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=7, stride=1, padding=3, bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch_out, ch_out, kernel_size=7, stride=1, padding=3, bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98e169d0-78ef-416a-9e3f-5209a58a8084",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block_9(nn.Module):\n",
    "    def __init_(self, ch_in, ch_out):\n",
    "        super(conv_block_9, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=9, stride=1, padding=4, bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch_out, ch_out, kernel_size=9, stride=1, padding=4, bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51351c9-eb00-4571-8260-f13aebc655a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_3_1(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(conv_3_1, self).__init__()\n",
    "\n",
    "        #self.conv_1 = conv_block_1(ch_in, ch_out)\n",
    "        #self.conv_2 = conv_block_2(ch_in, ch_out)\n",
    "        self.conv_3 = conv_block_3(ch_in, ch_out)\n",
    "        #self.conv_5 = conv_block_5(ch_in, ch_out)\n",
    "        self.conv_7 = conv_block_7(ch_in, ch_out)\n",
    "        #self.conv_9 = conv_block_9(ch_in, ch_out)\n",
    "\n",
    "        self.conv = nn.Conv2d(ch_out * 2, ch_out, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x1 = self.conv_1(x)\n",
    "        #x2 = self.conv_2(x)\n",
    "        x3 = self.conv_3(x)\n",
    "        #x5 = self.conv_5(x)\n",
    "        x7 = self.conv_7(x)\n",
    "        #x9 = self.conv_9(x)\n",
    "\n",
    "        x = torch.cat((x3, x7), dim=1)\n",
    "        x = self.conv(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2fdaab-7fa1-49c6-b9aa-5eadb7b920a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class up_conv(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out, bilinear = False):\n",
    "        super(up_conv, self).__init__()\n",
    "        if bilinear:\n",
    "            self.up = nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "                nn.Conv2d(ch_in, ch_in, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "                nn.BatchNorm2d(ch_in),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(ch_in, ch_out, kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.up(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb387d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class U_Net(nn.Module):\n",
    "    def __init__(self, img_ch=1, output_ch=1):\n",
    "        super(U_Net, self).__init__()\n",
    "\n",
    "        filters_number = [32, 64, 128, 256, 512]\n",
    "\n",
    "        self.Maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.Conv1 = conv_block(ch_in=img_ch, ch_out=filters_number[0])\n",
    "        self.Conv2 = conv_block(ch_in=filters_number[0], ch_out=filters_number[1])\n",
    "        self.Conv3 = conv_block(ch_in=filters_number[1], ch_out=filters_number[2])\n",
    "        self.Conv4 = conv_block(ch_in=filters_number[2], ch_out=filters_number[3])\n",
    "        self.Conv5 = conv_block(ch_in=filters_number[3], ch_out=filters_number[4])\n",
    "\n",
    "        self.Up5 = up_conv(ch_in=filters_number[4], ch_out=filters_number[3])\n",
    "        self.Up_conv5 = conv_block(ch_in=filters_number[4], ch_out=filters_number[3])\n",
    "\n",
    "        self.Up4 = up_conv(ch_in=filters_number[3], ch_out=filters_number[2])\n",
    "        self.Up_conv4 = conv_block(ch_in=filters_number[3], ch_out=filters_number[2])\n",
    "\n",
    "        self.Up3 = up_conv(ch_in=filters_number[2], ch_out=filters_number[1])\n",
    "        self.Up_conv3 = conv_block(ch_in=filters_number[2], ch_out=filters_number[1])\n",
    "\n",
    "        self.Up2 = up_conv(ch_in=filters_number[1], ch_out=filters_number[0])\n",
    "        self.Up_conv2 = conv_block(ch_in=filters_number[1], ch_out=filters_number[0])\n",
    "\n",
    "        self.Conv_1x1 = nn.Conv2d(filters_number[0], output_ch, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x1 = self.Conv1(x)\n",
    "\n",
    "        x2 = self.Maxpool(x1)\n",
    "        x2 = self.Conv2(x2)\n",
    "\n",
    "        x3 = self.Maxpool(x2)\n",
    "        x3 = self.Conv3(x3)\n",
    "\n",
    "        x4 = self.Maxpool(x3)\n",
    "        x4= self.Conv4(x4)\n",
    "\n",
    "        x5 = self.Maxpool(x4)\n",
    "        x5 = self.Conv5(x5)\n",
    "\n",
    "        d5 = self.Up5(x5)\n",
    "        d5 = torch.cat((x4, d5), dim=1)\n",
    "        d5 = self.Up_conv5(d5)\n",
    "\n",
    "        d4 = self.Up4(d5)\n",
    "        d4 = torch.cat((x3, d4), dim=1)\n",
    "        d4 = self.Up_conv4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        d3 = torch.cat((x2, d3), dim=1)\n",
    "        d3 = self.Up_conv3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        d2 = torch.cat((x1, d2), dim=1)\n",
    "        d2 = self.Up_conv2(d2)\n",
    "\n",
    "        d1 = self.Conv_1x1(d2)\n",
    "\n",
    "        return d1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
