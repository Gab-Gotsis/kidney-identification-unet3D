{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1213,
     "status": "ok",
     "timestamp": 1700283822131,
     "user": {
      "displayName": "Andrew Chin Chai",
      "userId": "11426134509903437449"
     },
     "user_tz": -660
    },
    "id": "M2jrjG-6Y4jy"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import nibabel as nib\n",
    "import matplotlib.pylab as plt\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from scipy import ndimage\n",
    "from datetime import datetime\n",
    "from glob import glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kg5v9ZnsqCk7"
   },
   "source": [
    "# 3D UNET\n",
    "- the model of 3D UNET is provided at <mark> from classes.models.unet3d import UNet3D </mark>\n",
    "They are few important parameters that are essential to extract features better.\n",
    "- The UNET model uses 3D convolution. It has 4 layers in the model.\n",
    "- Default kernel size for Double convolution is 3 or (3x3x3)\n",
    "- Number of features channels: Increase the number of channels for features enable prediction of classes.\n",
    "- channel selector 0: (4, 8, 16, 32, 64) has failed to extract any class but background.\n",
    "- channel selector 1: (8, 16, 32, 64, 128) can obtain segmentation for kidney well. However, features for cyst and tumor could not be successfully predicted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1700283822131,
     "user": {
      "displayName": "Andrew Chin Chai",
      "userId": "11426134509903437449"
     },
     "user_tz": -660
    },
    "id": "Fzr0fn45ZWja"
   },
   "outputs": [],
   "source": [
    "base_dir = \"./\"\n",
    "raw_dataset_dir = \"dataset/\"\n",
    "transformed_dataset_dir_path = \"dataset/affine_transformed/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1700283822131,
     "user": {
      "displayName": "Andrew Chin Chai",
      "userId": "11426134509903437449"
     },
     "user_tz": -660
    },
    "id": "8Uor3sVciNlr",
    "outputId": "f4c471b3-7588-40fa-bfdc-ba532c6f7503"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset folder exists, OK\n"
     ]
    }
   ],
   "source": [
    "is_colab = True\n",
    "if is_colab:\n",
    "    base_dir = \"/content/drive/MyDrive/Colab Notebooks/\"\n",
    "    if not os.path.isdir(base_dir):\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "\n",
    "raw_dataset_dir = os.path.join(base_dir, raw_dataset_dir)\n",
    "transformed_dataset_dir_path = os.path.join(base_dir, transformed_dataset_dir_path)\n",
    "\n",
    "if os.path.isdir(raw_dataset_dir) and os.path.isdir(transformed_dataset_dir_path):\n",
    "    print(\"dataset folder exists, OK\")\n",
    "else:\n",
    "    raise Exception(\"check path for dataset:{} \\n path for transformed dataset: {}\"\n",
    "                    .format(raw_dataset_dir, transformed_dataset_dir_path))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1221,
     "status": "ok",
     "timestamp": 1700283823349,
     "user": {
      "displayName": "Andrew Chin Chai",
      "userId": "11426134509903437449"
     },
     "user_tz": -660
    },
    "id": "WWiy5eIaia0G"
   },
   "outputs": [],
   "source": [
    "sys.path.append(base_dir)\n",
    "from classes.dataset_utils.toTorchDataset import ProcessedKit23TorchDataset\n",
    "from classes.models.unet3d import UNet3D\n",
    "from classes.epoch_results import EpochResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1700283823349,
     "user": {
      "displayName": "Andrew Chin Chai",
      "userId": "11426134509903437449"
     },
     "user_tz": -660
    },
    "id": "sL0Az8MVjTaX"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1700283823350,
     "user": {
      "displayName": "Andrew Chin Chai",
      "userId": "11426134509903437449"
     },
     "user_tz": -660
    },
    "id": "JFSy6pCNZTI7",
    "outputId": "4613c708-25f3-4bb1-ee75-e76c2a3f0a6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training data:366    size of testing dat:123\n"
     ]
    }
   ],
   "source": [
    "training_data = ProcessedKit23TorchDataset(train_data=True, test_size=0.25, dataset_dir =transformed_dataset_dir_path)\n",
    "test_data = ProcessedKit23TorchDataset(train_data=False, test_size=0.25, dataset_dir =transformed_dataset_dir_path)\n",
    "print(\"size of training data:{}    size of testing dat:{}\".format(len(training_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zuZEHGf9p5P_"
   },
   "source": [
    "## Reduce Training Cases and Test Cases\n",
    "- Following is used to reduce number of Training and Test casess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1700283823350,
     "user": {
      "displayName": "Andrew Chin Chai",
      "userId": "11426134509903437449"
     },
     "user_tz": -660
    },
    "id": "U5wsnRR-aZb_"
   },
   "outputs": [],
   "source": [
    "is_simplified = True\n",
    "# to demo, only 10 test cases are tested.\n",
    "if is_simplified:\n",
    "    training_data.case_dirs = training_data.case_dirs[:100]\n",
    "    training_data.case_names = training_data.case_names[:100]\n",
    "    test_data.case_dirs = test_data.case_dirs[:10]\n",
    "    test_data.case_names = test_data.case_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 1143,
     "status": "ok",
     "timestamp": 1700283824489,
     "user": {
      "displayName": "Andrew Chin Chai",
      "userId": "11426134509903437449"
     },
     "user_tz": -660
    },
    "id": "TRU4gVAJZOrd"
   },
   "outputs": [],
   "source": [
    "channel_selection = 2\n",
    "ks = 5\n",
    "model = UNet3D(1, 4, channel_selection=channel_selection, double_conv_kernel_size=ks).to(device)\n",
    "model._initialize_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9uwZcPUpg2q"
   },
   "source": [
    "## Optimizer or Gradient Descent Model\n",
    "- Enable choose of ADAM or SGD\n",
    "- Adjust learning rate decay manually. Higher gamma if there are high number of test data. For 100 cases, gamma 0.95 is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 1490,
     "status": "ok",
     "timestamp": 1700283825976,
     "user": {
      "displayName": "Andrew Chin Chai",
      "userId": "11426134509903437449"
     },
     "user_tz": -660
    },
    "id": "2gNMg32ZlBj-"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-3)\n",
    "is_ADAM = True\n",
    "if is_ADAM:\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1700283825977,
     "user": {
      "displayName": "Andrew Chin Chai",
      "userId": "11426134509903437449"
     },
     "user_tz": -660
    },
    "id": "rIyGyBRZlCI-",
    "outputId": "87e46111-d332-4fc2-9659-bee9503f7806"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unet3D - was initialised with weight\n"
     ]
    }
   ],
   "source": [
    "continue_from_checkpoint = False\n",
    "epoch_res = EpochResult()\n",
    "epoch_start = 0\n",
    "if continue_from_checkpoint:\n",
    "    print(\"Unet3D - loading from trained weight\")\n",
    "    checkpoint_ref_filepath = None\n",
    "    # this continues from certain training points\n",
    "    if is_ADAM:\n",
    "        checkpoint_ref_filepath = \"training_checkpoints/Model_UNET_epoch40.pth.tar\"\n",
    "    else:\n",
    "        checkpoint_ref_filepath = \"training_checkpoints/Model_UNET_SGD_epoch40.pth.tar\"\n",
    "    checkpoint_file = os.path.join(base_dir, checkpoint_ref_filepath)\n",
    "    checkpoint = torch.load(checkpoint_file)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    # load additional customised info from checkpoint\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    ep_list = checkpoint['epoch_list']\n",
    "    loss_list = checkpoint['loss_list']\n",
    "    lr_list = checkpoint['lr_list']\n",
    "    epoch_res = EpochResult(_epoch_list =ep_list, _loss_list=loss_list, _lr_list=lr_list)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    epoch_start = epoch_res.epoch_list[-1] + 1\n",
    "else:\n",
    "    print(\"Unet3D - was initialised with weight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUDLvdRNsKR0"
   },
   "source": [
    "## Training params\n",
    "Batch size used  \n",
    "- channel selector 0: batch size 6\n",
    "- channel selector 1: batch size 3\n",
    "- channel selector 2: batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1700283825977,
     "user": {
      "displayName": "Andrew Chin Chai",
      "userId": "11426134509903437449"
     },
     "user_tz": -660
    },
    "id": "tyEc7vbXjOOe"
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "total_batches = math.ceil(len(training_data) / batch_size)\n",
    "num_epochs = 100\n",
    "model_unet_save_path = os.path.join(base_dir,\"training_checkpoints/Model_UNET_ch{}_ks{}_epoch{}.pth.tar\")\n",
    "if not is_ADAM:\n",
    "    model_unet_save_path = os.path.join(base_dir,\"training_checkpoints/Model_UNET_ch{}_ks{}_SGD_epoch{}.pth.tar\")\n",
    "train_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7CVwkBgsy9L"
   },
   "source": [
    "## Training Loop\n",
    "- Cross validation during training is commented out. This is because training is extremely costly and the team has already used Colab GPU Tesla T4 for the task.\n",
    "- Please note that compute unit is not free in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2072939,
     "status": "error",
     "timestamp": 1700285898912,
     "user": {
      "displayName": "Andrew Chin Chai",
      "userId": "11426134509903437449"
     },
     "user_tz": -660
    },
    "id": "owyRGaKGrRgG",
    "outputId": "ff96783a-b557-4bf7-843c-d90c7994dbf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0/100 batch:0/100   Loss:1.9070  avg batch time:5.3 LR=0.001000\n",
      "Epoch:0/100 batch:5/100   Loss:1.1907  avg batch time:3.0 LR=0.001000\n",
      "Epoch:0/100 batch:10/100   Loss:0.7994  avg batch time:2.7 LR=0.001000\n",
      "Epoch:0/100 batch:15/100   Loss:0.6093  avg batch time:2.6 LR=0.001000\n",
      "Epoch:0/100 batch:20/100   Loss:0.6072  avg batch time:2.6 LR=0.001000\n",
      "Epoch:0/100 batch:25/100   Loss:0.4412  avg batch time:2.6 LR=0.001000\n",
      "Epoch:0/100 batch:30/100   Loss:0.4188  avg batch time:2.6 LR=0.001000\n",
      "Epoch:0/100 batch:35/100   Loss:0.3485  avg batch time:2.6 LR=0.001000\n",
      "Epoch:0/100 batch:40/100   Loss:0.3406  avg batch time:2.6 LR=0.001000\n",
      "Epoch:0/100 batch:45/100   Loss:0.3147  avg batch time:2.5 LR=0.001000\n",
      "Epoch:0/100 batch:50/100   Loss:0.3212  avg batch time:2.5 LR=0.001000\n",
      "Epoch:0/100 batch:55/100   Loss:0.2538  avg batch time:2.5 LR=0.001000\n",
      "Epoch:0/100 batch:60/100   Loss:0.2424  avg batch time:2.5 LR=0.001000\n",
      "Epoch:0/100 batch:65/100   Loss:0.2334  avg batch time:2.5 LR=0.001000\n",
      "Epoch:0/100 batch:70/100   Loss:0.2524  avg batch time:2.5 LR=0.001000\n",
      "Epoch:0/100 batch:75/100   Loss:0.2710  avg batch time:2.5 LR=0.001000\n",
      "Epoch:0/100 batch:80/100   Loss:0.2324  avg batch time:2.5 LR=0.001000\n",
      "Epoch:0/100 batch:85/100   Loss:0.1849  avg batch time:2.5 LR=0.001000\n",
      "Epoch:0/100 batch:90/100   Loss:0.1769  avg batch time:2.5 LR=0.001000\n",
      "Epoch:0/100 batch:95/100   Loss:0.1699  avg batch time:2.5 LR=0.001000\n",
      "Epoch:1/100 batch:0/100   Loss:0.1841  avg batch time:2.5 LR=0.000950\n",
      "Epoch:1/100 batch:5/100   Loss:0.1526  avg batch time:2.5 LR=0.000950\n",
      "Epoch:1/100 batch:10/100   Loss:0.1562  avg batch time:2.5 LR=0.000950\n",
      "Epoch:1/100 batch:15/100   Loss:0.1833  avg batch time:2.5 LR=0.000950\n",
      "Epoch:1/100 batch:20/100   Loss:0.1327  avg batch time:2.5 LR=0.000950\n",
      "Epoch:1/100 batch:25/100   Loss:0.1554  avg batch time:2.5 LR=0.000950\n",
      "Epoch:1/100 batch:30/100   Loss:0.1490  avg batch time:2.5 LR=0.000950\n",
      "Epoch:1/100 batch:35/100   Loss:0.1305  avg batch time:2.5 LR=0.000950\n",
      "Epoch:1/100 batch:40/100   Loss:0.2014  avg batch time:2.5 LR=0.000950\n",
      "Epoch:1/100 batch:45/100   Loss:0.1206  avg batch time:2.5 LR=0.000950\n",
      "Epoch:1/100 batch:50/100   Loss:0.1140  avg batch time:2.5 LR=0.000950\n",
      "Epoch:1/100 batch:55/100   Loss:0.1138  avg batch time:2.5 LR=0.000950\n",
      "Epoch:1/100 batch:60/100   Loss:0.0875  avg batch time:2.5 LR=0.000950\n",
      "Epoch:1/100 batch:65/100   Loss:0.1496  avg batch time:2.5 LR=0.000950\n",
      "Epoch:1/100 batch:70/100   Loss:0.0983  avg batch time:2.5 LR=0.000950\n",
      "Epoch:1/100 batch:75/100   Loss:0.1010  avg batch time:2.5 LR=0.000950\n",
      "Epoch:1/100 batch:80/100   Loss:0.1019  avg batch time:2.5 LR=0.000950\n",
      "Epoch:1/100 batch:85/100   Loss:0.0871  avg batch time:2.5 LR=0.000950\n",
      "Epoch:1/100 batch:90/100   Loss:0.1368  avg batch time:2.5 LR=0.000950\n",
      "Epoch:1/100 batch:95/100   Loss:0.0923  avg batch time:2.5 LR=0.000950\n",
      "Epoch:2/100 batch:0/100   Loss:0.0935  avg batch time:2.5 LR=0.000902\n",
      "Epoch:2/100 batch:5/100   Loss:0.0703  avg batch time:2.5 LR=0.000902\n",
      "Epoch:2/100 batch:10/100   Loss:0.0926  avg batch time:2.5 LR=0.000902\n",
      "Epoch:2/100 batch:15/100   Loss:0.0819  avg batch time:2.5 LR=0.000902\n",
      "Epoch:2/100 batch:20/100   Loss:0.0728  avg batch time:2.5 LR=0.000902\n",
      "Epoch:2/100 batch:25/100   Loss:0.0801  avg batch time:2.5 LR=0.000902\n",
      "Epoch:2/100 batch:30/100   Loss:0.0792  avg batch time:2.5 LR=0.000902\n",
      "Epoch:2/100 batch:35/100   Loss:0.1168  avg batch time:2.5 LR=0.000902\n",
      "Epoch:2/100 batch:40/100   Loss:0.0711  avg batch time:2.5 LR=0.000902\n",
      "Epoch:2/100 batch:45/100   Loss:0.0835  avg batch time:2.5 LR=0.000902\n",
      "Epoch:2/100 batch:50/100   Loss:0.0961  avg batch time:2.5 LR=0.000902\n",
      "Epoch:2/100 batch:55/100   Loss:0.0846  avg batch time:2.5 LR=0.000902\n",
      "Epoch:2/100 batch:60/100   Loss:0.0734  avg batch time:2.5 LR=0.000902\n",
      "Epoch:2/100 batch:65/100   Loss:0.0760  avg batch time:2.5 LR=0.000902\n",
      "Epoch:2/100 batch:70/100   Loss:0.0706  avg batch time:2.5 LR=0.000902\n",
      "Epoch:2/100 batch:75/100   Loss:0.0508  avg batch time:2.5 LR=0.000902\n",
      "Epoch:2/100 batch:80/100   Loss:0.0497  avg batch time:2.5 LR=0.000902\n",
      "Epoch:2/100 batch:85/100   Loss:0.0565  avg batch time:2.5 LR=0.000902\n",
      "Epoch:2/100 batch:90/100   Loss:0.0892  avg batch time:2.5 LR=0.000902\n",
      "Epoch:2/100 batch:95/100   Loss:0.0699  avg batch time:2.5 LR=0.000902\n",
      "Epoch:3/100 batch:0/100   Loss:0.0756  avg batch time:2.5 LR=0.000857\n",
      "Epoch:3/100 batch:5/100   Loss:0.0655  avg batch time:2.5 LR=0.000857\n",
      "Epoch:3/100 batch:10/100   Loss:0.0432  avg batch time:2.5 LR=0.000857\n",
      "Epoch:3/100 batch:15/100   Loss:0.0637  avg batch time:2.5 LR=0.000857\n",
      "Epoch:3/100 batch:20/100   Loss:0.0412  avg batch time:2.5 LR=0.000857\n",
      "Epoch:3/100 batch:25/100   Loss:0.0531  avg batch time:2.5 LR=0.000857\n",
      "Epoch:3/100 batch:30/100   Loss:0.0613  avg batch time:2.5 LR=0.000857\n",
      "Epoch:3/100 batch:35/100   Loss:0.0665  avg batch time:2.5 LR=0.000857\n",
      "Epoch:3/100 batch:40/100   Loss:0.0579  avg batch time:2.5 LR=0.000857\n",
      "Epoch:3/100 batch:45/100   Loss:0.0519  avg batch time:2.5 LR=0.000857\n",
      "Epoch:3/100 batch:50/100   Loss:0.0726  avg batch time:2.5 LR=0.000857\n",
      "Epoch:3/100 batch:55/100   Loss:0.0499  avg batch time:2.5 LR=0.000857\n",
      "Epoch:3/100 batch:60/100   Loss:0.0478  avg batch time:2.5 LR=0.000857\n",
      "Epoch:3/100 batch:65/100   Loss:0.0709  avg batch time:2.5 LR=0.000857\n",
      "Epoch:3/100 batch:70/100   Loss:0.0538  avg batch time:2.5 LR=0.000857\n",
      "Epoch:3/100 batch:75/100   Loss:0.0733  avg batch time:2.5 LR=0.000857\n",
      "Epoch:3/100 batch:80/100   Loss:0.0559  avg batch time:2.5 LR=0.000857\n",
      "Epoch:3/100 batch:85/100   Loss:0.1091  avg batch time:2.5 LR=0.000857\n",
      "Epoch:3/100 batch:90/100   Loss:0.0352  avg batch time:2.5 LR=0.000857\n",
      "Epoch:3/100 batch:95/100   Loss:0.0600  avg batch time:2.5 LR=0.000857\n",
      "Epoch:4/100 batch:0/100   Loss:0.0615  avg batch time:2.5 LR=0.000815\n",
      "Epoch:4/100 batch:5/100   Loss:0.0468  avg batch time:2.5 LR=0.000815\n",
      "Epoch:4/100 batch:10/100   Loss:0.0459  avg batch time:2.5 LR=0.000815\n",
      "Epoch:4/100 batch:15/100   Loss:0.0417  avg batch time:2.5 LR=0.000815\n",
      "Epoch:4/100 batch:20/100   Loss:0.0444  avg batch time:2.5 LR=0.000815\n",
      "Epoch:4/100 batch:25/100   Loss:0.0441  avg batch time:2.5 LR=0.000815\n",
      "Epoch:4/100 batch:30/100   Loss:0.0433  avg batch time:2.5 LR=0.000815\n",
      "Epoch:4/100 batch:35/100   Loss:0.0710  avg batch time:2.5 LR=0.000815\n",
      "Epoch:4/100 batch:40/100   Loss:0.0365  avg batch time:2.5 LR=0.000815\n",
      "Epoch:4/100 batch:45/100   Loss:0.1213  avg batch time:2.5 LR=0.000815\n",
      "Epoch:4/100 batch:50/100   Loss:0.0532  avg batch time:2.5 LR=0.000815\n",
      "Epoch:4/100 batch:55/100   Loss:0.0403  avg batch time:2.5 LR=0.000815\n",
      "Epoch:4/100 batch:60/100   Loss:0.0673  avg batch time:2.5 LR=0.000815\n",
      "Epoch:4/100 batch:65/100   Loss:0.0359  avg batch time:2.5 LR=0.000815\n",
      "Epoch:4/100 batch:70/100   Loss:0.0497  avg batch time:2.5 LR=0.000815\n",
      "Epoch:4/100 batch:75/100   Loss:0.0753  avg batch time:2.5 LR=0.000815\n",
      "Epoch:4/100 batch:80/100   Loss:0.0472  avg batch time:2.5 LR=0.000815\n",
      "Epoch:4/100 batch:85/100   Loss:0.0352  avg batch time:2.5 LR=0.000815\n",
      "Epoch:4/100 batch:90/100   Loss:0.0538  avg batch time:2.5 LR=0.000815\n",
      "Epoch:4/100 batch:95/100   Loss:0.0510  avg batch time:2.5 LR=0.000815\n",
      "Epoch:5/100 batch:0/100   Loss:0.0493  avg batch time:2.5 LR=0.000774\n",
      "Epoch:5/100 batch:5/100   Loss:0.0361  avg batch time:2.5 LR=0.000774\n",
      "Epoch:5/100 batch:10/100   Loss:0.0276  avg batch time:2.5 LR=0.000774\n",
      "Epoch:5/100 batch:15/100   Loss:0.0339  avg batch time:2.5 LR=0.000774\n",
      "Epoch:5/100 batch:20/100   Loss:0.0212  avg batch time:2.5 LR=0.000774\n",
      "Epoch:5/100 batch:25/100   Loss:0.0193  avg batch time:2.5 LR=0.000774\n",
      "Epoch:5/100 batch:30/100   Loss:0.0275  avg batch time:2.5 LR=0.000774\n",
      "Epoch:5/100 batch:35/100   Loss:0.0291  avg batch time:2.5 LR=0.000774\n",
      "Epoch:5/100 batch:40/100   Loss:0.0298  avg batch time:2.5 LR=0.000774\n",
      "Epoch:5/100 batch:45/100   Loss:0.0273  avg batch time:2.5 LR=0.000774\n",
      "Epoch:5/100 batch:50/100   Loss:0.0289  avg batch time:2.5 LR=0.000774\n",
      "Epoch:5/100 batch:55/100   Loss:0.0311  avg batch time:2.5 LR=0.000774\n",
      "Epoch:5/100 batch:60/100   Loss:0.0350  avg batch time:2.5 LR=0.000774\n",
      "Epoch:5/100 batch:65/100   Loss:0.0372  avg batch time:2.5 LR=0.000774\n",
      "Epoch:5/100 batch:70/100   Loss:0.0336  avg batch time:2.5 LR=0.000774\n",
      "Epoch:5/100 batch:75/100   Loss:0.0387  avg batch time:2.5 LR=0.000774\n",
      "Epoch:5/100 batch:80/100   Loss:0.0270  avg batch time:2.5 LR=0.000774\n",
      "Epoch:5/100 batch:85/100   Loss:0.0283  avg batch time:2.5 LR=0.000774\n",
      "Epoch:5/100 batch:90/100   Loss:0.0436  avg batch time:2.5 LR=0.000774\n",
      "Epoch:5/100 batch:95/100   Loss:0.0218  avg batch time:2.5 LR=0.000774\n",
      "Epoch:6/100 batch:0/100   Loss:0.0228  avg batch time:2.5 LR=0.000735\n",
      "Epoch:6/100 batch:5/100   Loss:0.0690  avg batch time:2.5 LR=0.000735\n",
      "Epoch:6/100 batch:10/100   Loss:0.0283  avg batch time:2.5 LR=0.000735\n",
      "Epoch:6/100 batch:15/100   Loss:0.0330  avg batch time:2.5 LR=0.000735\n",
      "Epoch:6/100 batch:20/100   Loss:0.0885  avg batch time:2.5 LR=0.000735\n",
      "Epoch:6/100 batch:25/100   Loss:0.0250  avg batch time:2.5 LR=0.000735\n",
      "Epoch:6/100 batch:30/100   Loss:0.0319  avg batch time:2.5 LR=0.000735\n",
      "Epoch:6/100 batch:35/100   Loss:0.0273  avg batch time:2.5 LR=0.000735\n",
      "Epoch:6/100 batch:40/100   Loss:0.0426  avg batch time:2.5 LR=0.000735\n",
      "Epoch:6/100 batch:45/100   Loss:0.0299  avg batch time:2.5 LR=0.000735\n",
      "Epoch:6/100 batch:50/100   Loss:0.0372  avg batch time:2.5 LR=0.000735\n",
      "Epoch:6/100 batch:55/100   Loss:0.0306  avg batch time:2.5 LR=0.000735\n",
      "Epoch:6/100 batch:60/100   Loss:0.0242  avg batch time:2.5 LR=0.000735\n",
      "Epoch:6/100 batch:65/100   Loss:0.0654  avg batch time:2.5 LR=0.000735\n",
      "Epoch:6/100 batch:70/100   Loss:0.0237  avg batch time:2.5 LR=0.000735\n",
      "Epoch:6/100 batch:75/100   Loss:0.0193  avg batch time:2.5 LR=0.000735\n",
      "Epoch:6/100 batch:80/100   Loss:0.0242  avg batch time:2.5 LR=0.000735\n",
      "Epoch:6/100 batch:85/100   Loss:0.0233  avg batch time:2.5 LR=0.000735\n",
      "Epoch:6/100 batch:90/100   Loss:0.0137  avg batch time:2.5 LR=0.000735\n",
      "Epoch:6/100 batch:95/100   Loss:0.0228  avg batch time:2.5 LR=0.000735\n",
      "Epoch:7/100 batch:0/100   Loss:0.0501  avg batch time:2.5 LR=0.000698\n",
      "Epoch:7/100 batch:5/100   Loss:0.0368  avg batch time:2.5 LR=0.000698\n",
      "Epoch:7/100 batch:10/100   Loss:0.0222  avg batch time:2.5 LR=0.000698\n",
      "Epoch:7/100 batch:15/100   Loss:0.0219  avg batch time:2.5 LR=0.000698\n",
      "Epoch:7/100 batch:20/100   Loss:0.0301  avg batch time:2.5 LR=0.000698\n",
      "Epoch:7/100 batch:25/100   Loss:0.0305  avg batch time:2.5 LR=0.000698\n",
      "Epoch:7/100 batch:30/100   Loss:0.0374  avg batch time:2.5 LR=0.000698\n",
      "Epoch:7/100 batch:35/100   Loss:0.0279  avg batch time:2.5 LR=0.000698\n",
      "Epoch:7/100 batch:40/100   Loss:0.0209  avg batch time:2.5 LR=0.000698\n",
      "Epoch:7/100 batch:45/100   Loss:0.0602  avg batch time:2.5 LR=0.000698\n",
      "Epoch:7/100 batch:50/100   Loss:0.0822  avg batch time:2.5 LR=0.000698\n",
      "Epoch:7/100 batch:55/100   Loss:0.0197  avg batch time:2.5 LR=0.000698\n",
      "Epoch:7/100 batch:60/100   Loss:0.0180  avg batch time:2.5 LR=0.000698\n",
      "Epoch:7/100 batch:65/100   Loss:0.0192  avg batch time:2.5 LR=0.000698\n",
      "Epoch:7/100 batch:70/100   Loss:0.0154  avg batch time:2.5 LR=0.000698\n",
      "Epoch:7/100 batch:75/100   Loss:0.0279  avg batch time:2.5 LR=0.000698\n",
      "Epoch:7/100 batch:80/100   Loss:0.0206  avg batch time:2.5 LR=0.000698\n",
      "Epoch:7/100 batch:85/100   Loss:0.0134  avg batch time:2.5 LR=0.000698\n",
      "Epoch:7/100 batch:90/100   Loss:0.0199  avg batch time:2.5 LR=0.000698\n",
      "Epoch:7/100 batch:95/100   Loss:0.0239  avg batch time:2.5 LR=0.000698\n",
      "Epoch:8/100 batch:0/100   Loss:0.0092  avg batch time:2.5 LR=0.000663\n",
      "Epoch:8/100 batch:5/100   Loss:0.0195  avg batch time:2.5 LR=0.000663\n",
      "Epoch:8/100 batch:10/100   Loss:0.0214  avg batch time:2.5 LR=0.000663\n",
      "Epoch:8/100 batch:15/100   Loss:0.0167  avg batch time:2.5 LR=0.000663\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-dbb54febe686>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_time_start = time.time()\n",
    "batches_per_epoch = len(train_loader)\n",
    "\n",
    "for epoch in range(epoch_start, num_epochs):\n",
    "    model.train()\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        images, masks = batch\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        masks = masks.long().squeeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images.float())\n",
    "        loss = criterion(outputs, masks)\n",
    "        running_loss = loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_processed_batches = (epoch - epoch_start) * batches_per_epoch + 1 + batch_idx\n",
    "        avg_batch_time = (time.time() - train_time_start) / total_processed_batches\n",
    "        if batch_idx % 5 == 0:\n",
    "            print(\"Epoch:{}/{} batch:{}/{}   Loss:{:.4f}  avg batch time:{:.1f} LR={:.6f}\".format(epoch, num_epochs, batch_idx, total_batches,running_loss, avg_batch_time, current_lr))\n",
    "    scheduler.step()\n",
    "    epoch_res.append_result(epoch, running_loss, current_lr)\n",
    "    model_checkpoint_path = model_unet_save_path.format(channel_selection, ks, epoch)\n",
    "    torch.save({'epoch_list': epoch_res.epoch_list, 'loss_list': epoch_res.loss_list,\n",
    "                'lr_list': epoch_res.lr_list, 'state_dict': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict()},model_checkpoint_path, _use_new_zipfile_serialization=True)\n",
    "    \n",
    "print('Finished Training')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNyVhcbr8IhIVkk5byyYE4b",
   "mount_file_id": "1GoF-_RRZBLN8DegMCNt95MTKRFsXu3tD",
   "provenance": [
    {
     "file_id": "10niTHkqNuQs02hCM9EaRSDmgLcKEFNPZ",
     "timestamp": 1700176080534
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
