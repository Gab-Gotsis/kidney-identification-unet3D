{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab6a1bf1-b10d-40d1-9928-a687d040472a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import onnx\n",
    "import sys\n",
    "import re\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy import ndimage\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49eca97c-cb70-45ec-90e1-145e00193f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')\n",
    "from classes.dataset_utils.toTorchDataset import ProcessedKit23TorchDataset\n",
    "from classes.models import resnet_model_generator\n",
    "from classes.config_class import ProjectModelResnetConfig\n",
    "from classes.epoch_results import EpochResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e237497-f4ec-43dd-ada2-d8e7090b2e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = ProcessedKit23TorchDataset(train_data=True, test_size=0.25, dataset_dir =\"./dataset/affine_transformed\")\n",
    "test_data = ProcessedKit23TorchDataset(train_data=False, test_size=0.25, dataset_dir =\"./dataset/affine_transformed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1645aa2c-21e5-4db1-902c-88d7f61ceaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_config = ProjectModelResnetConfig(model_depth=50)\n",
    "proj_resnet_model, _ = resnet_model_generator.generate_model(proj_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fda8f6d0-d98b-418b-abd5-179f9dcb1d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_config.set_net_model(proj_resnet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed96cd66-b261-44e1-a795-0954f1a6ee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "optimizer = optim.SGD(proj_config.nn_model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-3)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "if not proj_config.no_cuda:\n",
    "    criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd5c09cd-f423-48c1-ab70-c8ffc95218d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./training_checkpoints/Model_resnet_50_epoch5.pth.tar\n"
     ]
    }
   ],
   "source": [
    "train_from_pretrained = False\n",
    "epoch_res = EpochResult()\n",
    "epoch_start = 0\n",
    "if train_from_pretrained:\n",
    "    print(\"loading from pretrained Med3D model\")\n",
    "    if proj_config.model_depth == 10:\n",
    "        proj_config.load_med3d_pretrain_weigth(\"../pretrainedModel/resnet_10_23dataset.pth\")\n",
    "    elif proj_config.model_depth == 50:\n",
    "        proj_config.load_med3d_pretrain_weigth(\"../pretrainedModel/resnet_50_23dataset.pth\")\n",
    "    else:\n",
    "        raise Exception(\"Only depth 10 and 50 are used for now.\")\n",
    "else:\n",
    "    # this continues from certain training points\n",
    "    checkpoint, epoch_res = proj_config.load_weight_from_epoch(\"./training_checkpoints/Model_resnet_50_epoch5.pth.tar\")\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    epoch_start = epoch_res.epoch_list[-1] + 1\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15707a1a-4fa9-4085-859c-3d73d1d1883e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(training_data, batch_size=proj_config.batch_size, shuffle=True, num_workers=proj_config.num_workers, pin_memory=proj_config.pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55372d7-33d2-44c2-9d27-3698440d027c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current epoch=    6 Learning Rate=[0.001]\n",
      "Epoch:6 Batch:0 loss = 0.00232, avg_batch_time = 29.98519\n",
      "Epoch:6 Batch:50 loss = 0.00231, avg_batch_time = 23.71196\n",
      "Epoch:6 Batch:100 loss = 0.00224, avg_batch_time = 21.16570\n",
      "Epoch:6 Batch:150 loss = 0.00220, avg_batch_time = 19.81156\n",
      "Epoch:6 Batch:200 loss = 0.00218, avg_batch_time = 18.98279\n",
      "Epoch:6 Batch:250 loss = 0.00213, avg_batch_time = 18.47534\n",
      "Epoch:6 Batch:300 loss = 0.00209, avg_batch_time = 18.13857\n"
     ]
    }
   ],
   "source": [
    "train_time_start = time.time()\n",
    "batches_per_epoch = len(data_loader)\n",
    "\n",
    "for epoch in range(epoch_start, proj_config.max_epoch):\n",
    "    current_lr = scheduler.get_last_lr()\n",
    "    running_loss = None\n",
    "    print(\"current epoch={:5d} Learning Rate={}\".format(epoch, current_lr))\n",
    "    \n",
    "    for batch_idx, batch_data  in enumerate(data_loader):\n",
    "        imgs, segs = batch_data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        y_preds = proj_config.nn_model(imgs.float())\n",
    "\n",
    "        [n, _, z_size, y_size, x_size] = y_preds.shape\n",
    "\n",
    "        resized_segs = np.zeros([n, z_size, y_size, x_size])\n",
    "        for idx in range(n):\n",
    "            seg = segs[idx][0]\n",
    "            [ori_z, ori_y, ori_x] = seg.shape \n",
    "            scale = [z_size/ori_z, y_size/ori_y, x_size/ori_x]\n",
    "            this_affine = np.array([[scale[0], 0, 0],[0, scale[1], 0],[0, 0, scale[2]]])\n",
    "            resized_segs[idx] = ndimage.affine_transform(seg, this_affine, output_shape=resized_segs[idx].shape, cval=0)\n",
    "\n",
    "        resized_segs = torch.tensor(resized_segs).to(torch.int64)\n",
    "        loss = criterion(y_preds, resized_segs)\n",
    "        running_loss = loss.item()\n",
    "        loss.backward()                \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        total_processed_batches = (epoch - epoch_start) * batches_per_epoch + 1 + batch_idx\n",
    "        avg_batch_time = (time.time() - train_time_start) / total_processed_batches\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(\"Epoch:{} Batch:{} loss = {:.5f}, avg_batch_time = {:.5f}\".format(epoch, batch_idx, running_loss, avg_batch_time))\n",
    "    scheduler.step()\n",
    "    epoch_res.append_result(epoch, running_loss, current_lr)\n",
    "    model_checkpoint_path = proj_config.save_checkpoint_pathname(epoch, with_Datetime=False)\n",
    "    torch.save({'epoch_list': epoch_res.epoch_list, 'loss_list': epoch_res.loss_list, 'lr_list': epoch_res.lr_list, \n",
    "                'state_dict': proj_config.nn_model.state_dict(),'optimizer': optimizer.state_dict()},model_checkpoint_path, _use_new_zipfile_serialization=True)\n",
    "\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
