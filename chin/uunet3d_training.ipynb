{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f832030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "import re\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy import ndimage\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b20d099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')\n",
    "from classes.dataset_utils.toTorchDataset import ProcessedKit23TorchDataset\n",
    "from classes.models.unet3d import UNet3D\n",
    "from classes.config_class import ProjectModelResnetConfig\n",
    "from classes.epoch_results import EpochResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d74d0026",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = ProcessedKit23TorchDataset(train_data=True, test_size=0.25, dataset_dir =\"./dataset/affine_transformed\")\n",
    "test_data = ProcessedKit23TorchDataset(train_data=False, test_size=0.25, dataset_dir =\"./dataset/affine_transformed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e564a781",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_config = ProjectModelResnetConfig(model_depth=50)\n",
    "net = UNet3D(1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df200ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "if not proj_config.no_cuda:\n",
    "    criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "083459ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "neural network model is not set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly depth 10 and 50 are used for now.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# this continues from certain training points\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     checkpoint, epoch_res \u001b[38;5;241m=\u001b[39m \u001b[43mproj_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weight_from_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./training_checkpoints/Model_resnet_50_epoch5.pth.tar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     16\u001b[0m     epoch_start \u001b[38;5;241m=\u001b[39m epoch_res\u001b[38;5;241m.\u001b[39mepoch_list[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/UNSW/COMP9444_keen_beans/hayden/../classes/config_class.py:75\u001b[0m, in \u001b[0;36mProjectModelResnetConfig.load_weight_from_epoch\u001b[0;34m(self, model_epoch_pth)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_weight_from_epoch\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_epoch_pth: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_model:\n\u001b[0;32m---> 75\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneural network model is not set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mprint\u001b[39m(model_epoch_pth)\n\u001b[1;32m     78\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mException\u001b[0m: neural network model is not set"
     ]
    }
   ],
   "source": [
    "train_from_pretrained = False\n",
    "epoch_res = EpochResult()\n",
    "epoch_start = 0\n",
    "if train_from_pretrained:\n",
    "    print(\"loading from pretrained Med3D model\")\n",
    "    if proj_config.model_depth == 10:\n",
    "        proj_config.load_med3d_pretrain_weigth(\"../pretrainedModel/resnet_10_23dataset.pth\")\n",
    "    elif proj_config.model_depth == 50:\n",
    "        proj_config.load_med3d_pretrain_weigth(\"../pretrainedModel/resnet_50_23dataset.pth\")\n",
    "    else:\n",
    "        raise Exception(\"Only depth 10 and 50 are used for now.\")\n",
    "else:\n",
    "    # this continues from certain training points\n",
    "    checkpoint, epoch_res = proj_config.load_weight_from_epoch(\"./training_checkpoints/Model_resnet_50_epoch5.pth.tar\")\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    epoch_start = epoch_res.epoch_list[-1] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02ff358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(training_data, batch_size=proj_config.batch_size, shuffle=True, num_workers=proj_config.num_workers, pin_memory=proj_config.pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d57f87fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current epoch=    0 Learning Rate=[0.001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x107cb3e20>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1442, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py\", line 930, in wait\n",
      "    ready = selector.select(timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# forward + backward + optimize\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m y_preds \u001b[38;5;241m=\u001b[39m \u001b[43mproj_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m [n, _, z_size, y_size, x_size] \u001b[38;5;241m=\u001b[39m y_preds\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     20\u001b[0m resized_segs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros([n, z_size, y_size, x_size])\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "train_time_start = time.time()\n",
    "batches_per_epoch = len(data_loader)\n",
    "\n",
    "for epoch in range(epoch_start, proj_config.max_epoch):\n",
    "    current_lr = scheduler.get_last_lr()\n",
    "    running_loss = None\n",
    "    print(\"current epoch={:5d} Learning Rate={}\".format(epoch, current_lr))\n",
    "    \n",
    "    for batch_idx, batch_data  in enumerate(data_loader):\n",
    "        imgs, segs = batch_data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        y_preds = proj_config.nn_model(imgs.float())\n",
    "\n",
    "        [n, _, z_size, y_size, x_size] = y_preds.shape\n",
    "\n",
    "        resized_segs = np.zeros([n, z_size, y_size, x_size])\n",
    "        for idx in range(n):\n",
    "            seg = segs[idx][0]\n",
    "            [ori_z, ori_y, ori_x] = seg.shape \n",
    "            scale = [z_size/ori_z, y_size/ori_y, x_size/ori_x]\n",
    "            this_affine = np.array([[scale[0], 0, 0],[0, scale[1], 0],[0, 0, scale[2]]])\n",
    "            resized_segs[idx] = ndimage.affine_transform(seg, this_affine, output_shape=resized_segs[idx].shape, cval=0)\n",
    "\n",
    "        resized_segs = torch.tensor(resized_segs).to(torch.int64)\n",
    "        loss = criterion(y_preds, resized_segs)\n",
    "        running_loss = loss.item()\n",
    "        loss.backward()                \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        total_processed_batches = (epoch - epoch_start) * batches_per_epoch + 1 + batch_idx\n",
    "        avg_batch_time = (time.time() - train_time_start) / total_processed_batches\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(\"Epoch:{} Batch:{} loss = {:.5f}, avg_batch_time = {:.5f}\".format(epoch, batch_idx, running_loss, avg_batch_time))\n",
    "    scheduler.step()\n",
    "    epoch_res.append_result(epoch, running_loss, current_lr)\n",
    "    model_checkpoint_path = proj_config.save_checkpoint_pathname(epoch, with_Datetime=False)\n",
    "    torch.save({'epoch_list': epoch_res.epoch_list, 'loss_list': epoch_res.loss_list, 'lr_list': epoch_res.lr_list, \n",
    "                'state_dict': proj_config.nn_model.state_dict(),'optimizer': optimizer.state_dict()},model_checkpoint_path, _use_new_zipfile_serialization=True)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60691a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
